{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c973fcc-0bba-44c3-853c-8b56effc5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JWST Data Quality Flags: Bit Position to Mnemonic Name\n",
    "dq_flags = {\n",
    "    0: \"DO\\_NOT\\_USE\",           # 2^0 = 1\n",
    "    1: \"SATURATED\",            # 2^1 = 2\n",
    "    2: \"JUMP\\_DET\",             # 2^2 = 4\n",
    "    3: \"DROPOUT\",              # 2^3 = 8\n",
    "    4: \"OUTLIER\",              # 2^4 = 16\n",
    "    5: \"PERSISTENCE\",          # 2^5 = 32\n",
    "    6: \"AD\\_FLOOR\",             # 2^6 = 64\n",
    "    7: \"CHARGELOSS\",           # 2^7 = 128\n",
    "    8: \"UNRELIABLE\\_ERROR\",     # 2^8 = 256\n",
    "    9: \"NON\\_SCIENCE\",          # 2^9 = 512\n",
    "    10: \"DEAD\",                # 2^10 = 1024\n",
    "    11: \"HOT\",                 # 2^11 = 2048\n",
    "    12: \"WARM\",                # 2^12 = 4096\n",
    "    13: \"LOW\\_QE\",              # 2^13 = 8192\n",
    "    14: \"RC\",                  # 2^14 = 16384\n",
    "    15: \"TELEGRAPH\",           # 2^15 = 32768\n",
    "    16: \"NONLINEAR\",           # 2^16 = 65536\n",
    "    17: \"BAD\\_REF_PIXEL\",       # 2^17 = 131072\n",
    "    18: \"NO\\_FLAT\\_FIELD\",       # 2^18 = 262144\n",
    "    19: \"NO\\_GAIN\\_VALUE\",       # 2^19 = 524288\n",
    "    20: \"NO\\_LIN\\_CORR\",         # 2^20 = 1048576\n",
    "    21: \"NO\\_SAT\\_CHECK\",        # 2^21 = 2097152\n",
    "    22: \"UNRELIABLE\\_BIAS\",     # 2^22 = 4194304\n",
    "    23: \"UNRELIABLE\\_DARK\",     # 2^23 = 8388608\n",
    "    24: \"UNRELIABLE\\_SLOPE\",    # 2^24 = 16777216\n",
    "    25: \"UNRELIABLE\\_FLAT\",     # 2^25 = 33554432\n",
    "    26: \"OPEN\",                # 2^26 = 67108864\n",
    "    27: \"ADJ\\_OPEN\",            # 2^27 = 134217728\n",
    "    28: \"FLUX\\_ESTIMATED\",      # 2^28 = 268435456\n",
    "    29: \"MSA\\_FAILED\\_OPEN\",     # 2^29 = 536870912\n",
    "    30: \"OTHER\\_BAD\\_PIXEL\",     # 2^30 = 1073741824\n",
    "    31: \"REFERENCE\\_PIXEL\"      # 2^31 = 2147483648\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f187a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from astropy.io import fits\n",
    "import concurrent.futures\n",
    "import shutil\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.ndimage import uniform_filter, label\n",
    "import warnings\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import scienceplots\n",
    "plt.style.use(['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "project_no = 1224\n",
    "observation = 3\n",
    "nrs_no = 2\n",
    "\n",
    "#f'../Data/data_0{project_no}/Obs00{observation}/uncal/' + \n",
    "\n",
    "ramp_files_modified = sorted(glob.glob(f'../Data/data_0{project_no}/Obs00{observation}/uncal/' + f'*jw0{project_no}*' + f'*nrs{nrs_no}*' + '*ramp_m*'))\n",
    "print(ramp_files_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ec6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fits.open(ramp_files_modified[0]) as hdul:\n",
    "    raw_data = hdul[3].data  # Assuming shape is already (150, 60, height, width)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Raw data shape:\", raw_data.shape)\n",
    "\n",
    "data_cube = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb2acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import os\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_counts(data_cube):\n",
    "    \"\"\"\n",
    "    Use Numba to compute bincounts per value over time steps.\n",
    "    data_cube shape: (N, T, H, W)\n",
    "    Returns: (32, T) array of counts\n",
    "    \"\"\"\n",
    "    N, T, H, W = data_cube.shape\n",
    "    counts_per_value = np.zeros((32, T), dtype=np.int64)\n",
    "\n",
    "    for t in prange(T):\n",
    "        temp = np.zeros(32, dtype=np.int64)\n",
    "        for n in range(N):\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    val = data_cube[n, t, h, w]\n",
    "                    temp[val] += 1\n",
    "        for v in range(32):\n",
    "            counts_per_value[v, t] = temp[v]\n",
    "\n",
    "    return counts_per_value\n",
    "\n",
    "def plot_comparison_across_files(file_paths, labels=None, colors=None):\n",
    "    \"\"\"\n",
    "    Compare histogram value trends over time steps from multiple FITS files.\n",
    "    \"\"\"\n",
    "    assert len(file_paths) > 0, \"Provide at least one FITS file path.\"\n",
    "    if labels is None:\n",
    "        labels = [os.path.basename(fp).replace('.fits', '') for fp in file_paths]\n",
    "    if colors is None:\n",
    "        colors = ['blue', 'red', 'green', 'olive', 'orange', 'cyan', 'purple', 'brown']\n",
    "\n",
    "    counts_all_files = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with fits.open(file_path) as hdul:\n",
    "            data_cube = hdul[3].data.astype(np.int64)  # Ensure integer type for Numba\n",
    "            counts_per_value = compute_counts(data_cube)\n",
    "            counts_all_files.append(counts_per_value)\n",
    "\n",
    "    total_counts = sum(counts for counts in counts_all_files)\n",
    "    nonzero_values = np.where(total_counts.sum(axis=1) > 0)[0]\n",
    "    nonzero_values = nonzero_values[nonzero_values != 0]  # skip zero if desired\n",
    "\n",
    "    num_timesteps = counts_all_files[0].shape[1]\n",
    "    num_plots = len(nonzero_values)\n",
    "    cols = 4\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 3))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, value in enumerate(nonzero_values):\n",
    "        ax = axes[idx]\n",
    "        for file_idx, counts in enumerate(counts_all_files):\n",
    "            ax.plot(\n",
    "                counts[value],\n",
    "                label=labels[file_idx].split('01-')[1][:11],\n",
    "                color=colors[file_idx % len(colors)]\n",
    "            )\n",
    "        ax.set_title(f\"Pixel type: $\\\\texttt{{{dq_flags.get(value, str(value))}}}$\")\n",
    "        ax.set_xlim(0, num_timesteps - 1)\n",
    "        ax.set_xticks([0, num_timesteps//2, num_timesteps - 1])\n",
    "        ax.set_xlabel('Group')\n",
    "        ax.set_ylabel('Count')\n",
    "\n",
    "    for j in range(num_plots, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # Shared legend outside plot area\n",
    "    fig.legend(\n",
    "        labels=[label.split('01-')[1][:11].replace('_', r'\\_') for label in labels],\n",
    "        loc='center right',\n",
    "        fontsize='small',\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # Make space on right for legend\n",
    "    plt.savefig(f'Data0{project_no}Obs00{observation}nrs{nrs_no}_count.pdf')\n",
    "    plt.show()\n",
    "\n",
    "# Call your function as before\n",
    "plot_comparison_across_files(ramp_files_modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d88455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from astropy.io import fits\n",
    "from numba import njit, prange\n",
    "\n",
    "# --- Numba-accelerated histogram function ---\n",
    "@njit(parallel=True)\n",
    "def compute_counts_per_value(data_cube):\n",
    "    \"\"\"\n",
    "    Compute histogram counts for values 0â€“31 across all time steps.\n",
    "    data_cube shape: (slices, time, height, width)\n",
    "    Returns: array of shape (32, num_timesteps)\n",
    "    \"\"\"\n",
    "    slices, num_timesteps, height, width = data_cube.shape\n",
    "    counts_per_value = np.zeros((32, num_timesteps), dtype=np.int64)\n",
    "\n",
    "    for t in prange(num_timesteps):\n",
    "        temp_counts = np.zeros(32, dtype=np.int64)\n",
    "        for s in range(slices):\n",
    "            for h in range(height):\n",
    "                for w in range(width):\n",
    "                    val = data_cube[s, t, h, w]\n",
    "                    temp_counts[val] += 1\n",
    "        for v in range(32):\n",
    "            counts_per_value[v, t] = temp_counts[v]\n",
    "\n",
    "    return counts_per_value\n",
    "\n",
    "\n",
    "# --- Main code execution ---\n",
    "with fits.open(ramp_files_modified[0]) as hdul:\n",
    "    data_cube = hdul[3].data.astype(np.int64)  # Ensure int64 for Numba compatibility\n",
    "\n",
    "num_slices, num_timesteps, height, width = data_cube.shape\n",
    "print(f\"Data cube shape: {data_cube.shape}\")\n",
    "\n",
    "# Compute histogram counts using optimized function\n",
    "counts_per_value = compute_counts_per_value(data_cube)\n",
    "\n",
    "# Identify nonzero pixel values (excluding 0 if desired)\n",
    "nonzero_values = np.where(counts_per_value.sum(axis=1) > 0)[0]\n",
    "nonzero_values = nonzero_values[nonzero_values != 0]\n",
    "num_plots = len(nonzero_values)\n",
    "\n",
    "# Set up plot grid\n",
    "cols = 4\n",
    "rows = int(np.ceil(num_plots / cols))\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 2.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot bar charts\n",
    "for idx, value in enumerate(nonzero_values):\n",
    "    ax = axes[idx]\n",
    "    ax.bar(\n",
    "        range(num_timesteps),\n",
    "        counts_per_value[value],\n",
    "        color=(mcolors.to_rgba('blue', alpha=0.3)),\n",
    "        edgecolor=mcolors.to_rgba('blue', alpha=0.7),\n",
    "        linewidth=1\n",
    "    )\n",
    "    ax.set_title(f\"Value {value}\")\n",
    "    ax.set_xlim(0, num_timesteps - 1)\n",
    "    ax.set_xticks([0, num_timesteps // 2, num_timesteps - 1])\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(num_plots, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad73aefc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "from astropy.io import fits\n",
    "import os\n",
    "\n",
    "# Numba-accelerated function to detect streaks allowing a single zero or 30 in between\n",
    "@njit\n",
    "def get_streaks_allowing_single_zero_numba(arr, target_value):\n",
    "    n = arr.shape[0]\n",
    "    max_streaks = n  # max possible streaks\n",
    "    starts = np.empty(max_streaks, dtype=np.int32)\n",
    "    ends = np.empty(max_streaks, dtype=np.int32)\n",
    "    lengths = np.empty(max_streaks, dtype=np.int32)\n",
    "\n",
    "    count = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        if arr[i] == target_value:\n",
    "            start = i\n",
    "            length = 1\n",
    "            gaps = 0\n",
    "            i += 1\n",
    "\n",
    "            while i < n:\n",
    "                if arr[i] == target_value:\n",
    "                    length += 1\n",
    "                elif (arr[i] == 0 or arr[i] == 30) and (i + 1 < n and arr[i + 1] == target_value) and gaps == 0:\n",
    "                    length += 1  # count zero as part of streak\n",
    "                    gaps += 1\n",
    "                    i += 1  # skip the zero\n",
    "                    length += 1  # count next target_value\n",
    "                else:\n",
    "                    break\n",
    "                i += 1\n",
    "\n",
    "            end = start + length - 1\n",
    "            starts[count] = start\n",
    "            ends[count] = end\n",
    "            lengths[count] = length\n",
    "            count += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return starts[:count], ends[:count], lengths[:count]\n",
    "\n",
    "# Numba-parallelized function to process a single slice and find streaks for one target value\n",
    "@njit(parallel=True)\n",
    "def process_slice_for_value_numba(data_cube_slice, target_value):\n",
    "    # data_cube_slice shape: (time, rows, cols)\n",
    "    time_len, rows, cols = data_cube_slice.shape\n",
    "\n",
    "    max_streaks = time_len * rows * cols  # pessimistic max\n",
    "    starts = np.empty(max_streaks, dtype=np.int32)\n",
    "    ends = np.empty(max_streaks, dtype=np.int32)\n",
    "    lengths = np.empty(max_streaks, dtype=np.int32)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for col in prange(cols):\n",
    "        for row in range(rows):\n",
    "            streak_array = data_cube_slice[:, row, col]\n",
    "            # Check if target_value present\n",
    "            found = False\n",
    "            for t in range(time_len):\n",
    "                if streak_array[t] == target_value:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                continue\n",
    "\n",
    "            s, e, l = get_streaks_allowing_single_zero_numba(streak_array, target_value)\n",
    "            for idx in range(s.shape[0]):\n",
    "                if count < max_streaks:\n",
    "                    starts[count] = s[idx]\n",
    "                    ends[count] = e[idx]\n",
    "                    lengths[count] = l[idx]\n",
    "                    count += 1\n",
    "\n",
    "    return starts[:count], ends[:count], lengths[:count]\n",
    "\n",
    "def analyze_streaks_across_files_numba(file_paths, nonzero_values, labels=None):\n",
    "    \"\"\"\n",
    "    Analyze streaks of target values across columns and slices in multiple FITS files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_paths: list of str, paths to FITS files.\n",
    "    - nonzero_values: list or array of int values to search for streaks.\n",
    "    - labels: optional list of labels corresponding to file_paths.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with structure:\n",
    "      {\n",
    "          'label1': {value1: streaks_array, value2: streaks_array, ...},\n",
    "          'label2': {...},\n",
    "          ...\n",
    "      }\n",
    "    Each streaks_array is a numpy structured array with fields ('start', 'end', 'length').\n",
    "    \"\"\"\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [os.path.basename(fp).replace('.fits', '') for fp in file_paths]\n",
    "\n",
    "    results_by_file = {}\n",
    "\n",
    "    for file_path, label in zip(file_paths, labels):\n",
    "        print(f\"\\nðŸ“‚ Processing file: {label}\")\n",
    "\n",
    "        with fits.open(file_path) as hdul:\n",
    "            data_cube = hdul[3].data  # shape (num_slices, time, rows, cols)\n",
    "            num_slices = data_cube.shape[0]\n",
    "\n",
    "        file_results = {}\n",
    "        for val in nonzero_values:\n",
    "            print(f\"  âž¤ Target value {val}...\")\n",
    "            all_starts = []\n",
    "            all_ends = []\n",
    "            all_lengths = []\n",
    "\n",
    "            for slice_idx in range(num_slices):\n",
    "                data_cube_slice = data_cube[slice_idx]  # shape (time, rows, cols)\n",
    "                s, e, l = process_slice_for_value_numba(data_cube_slice, val)\n",
    "                all_starts.append(s)\n",
    "                all_ends.append(e)\n",
    "                all_lengths.append(l)\n",
    "\n",
    "            # Concatenate arrays for all slices\n",
    "            if all_starts:\n",
    "                all_starts = np.concatenate(all_starts)\n",
    "                all_ends = np.concatenate(all_ends)\n",
    "                all_lengths = np.concatenate(all_lengths)\n",
    "            else:\n",
    "                all_starts = np.array([], dtype=np.int32)\n",
    "                all_ends = np.array([], dtype=np.int32)\n",
    "                all_lengths = np.array([], dtype=np.int32)\n",
    "\n",
    "            # Create structured array for easy use\n",
    "            streaks_array = np.empty(all_starts.shape[0], dtype=[('start', np.int32), ('end', np.int32), ('length', np.int32)])\n",
    "            streaks_array['start'] = all_starts\n",
    "            streaks_array['end'] = all_ends\n",
    "            streaks_array['length'] = all_lengths\n",
    "\n",
    "            file_results[val] = streaks_array\n",
    "\n",
    "        results_by_file[label] = file_results\n",
    "\n",
    "    return results_by_file\n",
    "\n",
    "# Call the Numba-optimized function\n",
    "results_by_file = analyze_streaks_across_files_numba(\n",
    "    ramp_files_modified,\n",
    "    nonzero_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the FITS file and display the header and XPOSURE value\n",
    "with fits.open(ramp_files_modified[0]) as hdul:\n",
    "    header = hdul[0].header  # Access the primary HDU header\n",
    "    tgroup_value = header.get('TGROUP', 'TGROUP key not found')\n",
    "    print(\"TGROUP value:\", tgroup_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c094d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Store all percentages for averaging\n",
    "\n",
    "n_plots = len(nonzero_values)\n",
    "n_rows = 4\n",
    "n_cols = int(np.ceil(n_plots / n_rows))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, val in enumerate(nonzero_values):\n",
    "    ax = axes[i]\n",
    "    all_percentages = {}\n",
    "\n",
    "    for files in ramp_files_modified:\n",
    "        data = results_by_file[files.split('/')[-1].replace('.fits', '')][val]\n",
    "    \n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        else: \n",
    "            data = data['length']\n",
    "            data = data[data > 0]  # Filter out zero lengths\n",
    "            data = data[data < 70]  # Filter out very long lengths\n",
    "        unique, counts = np.unique(data, return_counts=True)\n",
    "        percentages = counts / counts.sum()\n",
    "        \n",
    "        for u, p in zip(unique, percentages):\n",
    "            if u not in all_percentages:\n",
    "                all_percentages[u] = []\n",
    "            all_percentages[u].append(p)\n",
    "\n",
    "    # Sort keys and compute average and std dev\n",
    "    sorted_keys = sorted(all_percentages.keys())\n",
    "    averages = np.array([np.mean(all_percentages[k]) for k in sorted_keys])\n",
    "    stds = np.array([np.std(all_percentages[k]) for k in sorted_keys])\n",
    "\n",
    "    # Plotting\n",
    "    ax.bar(sorted_keys, averages, yerr=stds, capsize=2, color='skyblue', edgecolor='blue', alpha=0.7, label='Average with Std Dev')\n",
    "    ax.set_xlabel(r'$N_\\text{group}$ unreliability length')\n",
    "    ax.set_ylabel('Percentage occurrence')\n",
    "    ax.set_title(f\"Pixel type: $\\\\texttt{{{dq_flags[val]}}}$\")\n",
    "\n",
    "# Remove empty plots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "plt.savefig(f'Data0{project_no}Obs00{observation}nrs{nrs_no}_bar_plot.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c848cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def exponential(x, a, b):\n",
    "    return a * np.exp(-b * (x - 1))\n",
    "\n",
    "fit_results = []  # To store (val, file, popt, r2score)\n",
    "\n",
    "nonzero_values_fix = [11, 13, 26, 27, 30]\n",
    "\n",
    "n_plots = len(nonzero_values_fix)\n",
    "n_rows = 3\n",
    "n_cols = int(np.ceil(n_plots / n_rows))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, val in enumerate(nonzero_values_fix):\n",
    "    ax = axes[i]\n",
    "    for files in ramp_files_modified:\n",
    "        data = results_by_file[files.split('/')[-1].replace('.fits', '')][val]\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        data = data['length']\n",
    "        data = data[data > 0]\n",
    "        data = data[data < 70]\n",
    "        unique, counts = np.unique(data, return_counts=True)\n",
    "        percentages = counts / counts.sum()\n",
    "        \n",
    "        # Exclude the last value for fitting\n",
    "        unique_fit = unique[:-1] * tgroup_value\n",
    "        percentages_fit = percentages[:-1]\n",
    "        \n",
    "        try:\n",
    "            popt, pcov = curve_fit(exponential, unique_fit, percentages_fit)\n",
    "            perr = np.sqrt(np.diag(pcov))  # Standard deviations of the parameters\n",
    "            r2 = r2_score(percentages_fit, exponential(unique_fit, *popt))\n",
    "            fit_results.append([popt[0], popt[1], r2])\n",
    "            ax.bar(unique * tgroup_value, percentages)\n",
    "            ax.plot(\n",
    "                unique_fit, \n",
    "                exponential(unique_fit, *popt), \n",
    "                label=(\n",
    "                    f'$a={popt[0]:.2f} \\pm {perr[0]:.2f}, '\n",
    "                    f'b={popt[1]:.2f} \\pm {perr[1]:.2f}, '\n",
    "                    f'R^2={r2:.2f}$'\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    ax.set_xlabel(r'Length of time pixel stayed unreliable [s]')\n",
    "    ax.set_ylabel('Percentage occurrence')\n",
    "    ax.set_title(f\"Pixel type: {dq_flags[val]}\")\n",
    "    ax.legend()\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acab51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Create the table as before\n",
    "grouped = [fit_results[i:i+2] for i in range(0, len(fit_results), 2)]\n",
    "table_data = []\n",
    "for label, group in zip(nonzero_values_fix, grouped):\n",
    "    a_vals = [arr[0] * 100 for arr in group] \n",
    "    b_vals = [arr[1] for arr in group]\n",
    "    r2_vals = [arr[2] for arr in group]\n",
    "    table_data.append([\n",
    "        f\"$\\\\texttt{{{dq_flags[label]}}}$\",\n",
    "        f\"{np.mean(a_vals):.3f} $\\pm$ {np.std(a_vals):.3f}\",\n",
    "        f\"{np.mean(b_vals):.3f} $\\pm$ {np.std(b_vals):.3f}\",\n",
    "        f\"{np.mean(r2_vals):.3f} $\\pm$ {np.std(r2_vals):.3f}\"\n",
    "    ])\n",
    "\n",
    "# Add overall mean row\n",
    "all_a = [arr[0] * 100 for arr in fit_results]\n",
    "all_b = [arr[1] for arr in fit_results]\n",
    "all_r2 = [arr[2] for arr in fit_results]\n",
    "table_data.append([\n",
    "    \"Overall mean\",\n",
    "    f\"{np.mean(all_a):.3f} $\\pm$ {np.std(all_a):.3f}\",\n",
    "    f\"{np.mean(all_b):.3f} $\\pm$ {np.std(all_b):.3f}\",\n",
    "    f\"{np.mean(all_r2):.3f} $\\pm$ {np.std(all_r2):.3f}\"\n",
    "])\n",
    "\n",
    "headers = [\"Pixel type\", \"A mean $\\pm$ 1$\\sigma$ [%]\", \"k mean $\\pm$ 1$\\sigma$ [s$^{-1}$]\", f\"$R^2$ mean $\\pm$ 1$\\sigma$\"]\n",
    "latex_table = tabulate(table_data, headers, tablefmt=\"latex\")\n",
    "with open(f\"Data0{project_no}Obs00{observation}nrs{nrs_no}_fit_results_table.txt\", \"w\") as f:\n",
    "    f.write(latex_table)\n",
    "with open(f\"Data0{project_no}Obs00{observation}nrs{nrs_no}_fit_results_table_np.txt\", \"w\") as f:\n",
    "    f.write(tabulate(table_data, headers))\n",
    "    \n",
    "tabulate(table_data, headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e69cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the LaTeX table from file\n",
    "with open(f\"Data0{project_no}Obs00{observation}nrs{nrs_no}_fit_results_table.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Replace all occurrences of \\textbackslash{}\n",
    "cleaned_content = content.replace(\"textbackslash{}\", \"\")\n",
    "cleaned_content = cleaned_content.replace(\"\\\\$\", \"$\")\n",
    "cleaned_content = cleaned_content.replace(\"\\^{}\", \"^\")\n",
    "cleaned_content = cleaned_content.replace(\"\\{\", \"{\")\n",
    "cleaned_content = cleaned_content.replace(\"\\}\", \"}\")\n",
    "cleaned_content = cleaned_content.replace(\"\\\\\\_\", \"\\\\_\")\n",
    "\n",
    "# Write the cleaned content back to the file (or to a new file)\n",
    "with open(f\"Data0{project_no}Obs00{observation}nrs{nrs_no}_fit_results_table.txt\", \"w\") as f:\n",
    "    f.write(cleaned_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a50f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "A_values = [37.284, 33.633, 38.144, 32.028]\n",
    "A_err = [5.454, 6.762, 5.452, 6.275]\n",
    "k_values = [0.831, 0.790, 0.907, 0.754]\n",
    "k_err = [0.166, 0.173, 0.201, 0.091]\n",
    "files = ['Data01224Obs002nrs1', 'Data01224Obs002nrs2', 'Data01224Obs003nrs1', 'Data01224Obs003nrs2']\n",
    "\n",
    "# Plot A values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(files, A_values, yerr=A_err, fmt='o', color='blue', ecolor='black', capsize=2)\n",
    "plt.ylabel('A Value (%)')\n",
    "plt.title('A Values with Error Bars')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot k values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(files, k_values, yerr=k_err, fmt='o', color='green', ecolor='black', capsize=2)\n",
    "plt.ylabel('k Value')\n",
    "plt.title('k Values with Error Bars')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93171a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = sorted(glob.glob(f'*_table_np.txt'))\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "tables = sorted(glob.glob('*_table_np.txt'))\n",
    "\n",
    "filename = tables[0]\n",
    "df = pd.read_fwf(filename)\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_latex_texttt(text):\n",
    "    # Remove $ and \\texttt{} wrappers\n",
    "    # Example: $\\texttt{HOT}$ -> HOT\n",
    "    if isinstance(text, str):\n",
    "        # Remove $...$\n",
    "        text = re.sub(r'\\$([^$]+)\\$', r'\\1', text)\n",
    "        # Remove \\texttt{...}\n",
    "        text = re.sub(r'\\\\texttt\\{([^}]+)\\}', r'\\1', text)\n",
    "        return text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleanup to the first column (Pixel type)\n",
    "df.iloc[:, 0] = df.iloc[:, 0].apply(clean_latex_texttt)\n",
    "\n",
    "df['Pixel type'] = df['Pixel type'].apply(clean_latex_texttt)\n",
    "df = df[1:]\n",
    "\n",
    "# Columns to split\n",
    "cols_to_split = [\n",
    "    'A mean $\\pm$ 1$\\sigma$ [%]',\n",
    "    'k mean $\\pm$ 1$\\sigma$ [s$^{-1}$]',\n",
    "    '$R^2$ mean $\\pm$ 1$\\sigma$'\n",
    "]\n",
    "\n",
    "for col in cols_to_split:\n",
    "    # Split on the literal '$\\pm$' (escaped)\n",
    "    new_cols = df[col].str.split(r'\\s*\\$\\\\pm\\$\\s*', expand=True)\n",
    "    df[col.replace(' mean $\\pm$ 1$\\sigma$', '_mean')] = new_cols[0].astype(float)\n",
    "    df[col.replace(' mean $\\pm$ 1$\\sigma$', '_std')] = new_cols[1].astype(float)\n",
    "\n",
    "# Drop the original combined columns if you want\n",
    "df = df.drop(columns=cols_to_split)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f01fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_latex_texttt(text):\n",
    "    # Remove $ and \\texttt{} wrappers, e.g. $\\texttt{HOT}$ -> HOT\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'\\$([^$]+)\\$', r'\\1', text)\n",
    "        text = re.sub(r'\\\\texttt\\{([^}]+)\\}', r'\\1', text)\n",
    "        return text.strip()\n",
    "    return text\n",
    "\n",
    "def process_table(filename):\n",
    "    df = pd.read_fwf(filename)\n",
    "    \n",
    "    # Clean first column Pixel type\n",
    "    df.iloc[:, 0] = df.iloc[:, 0].apply(clean_latex_texttt)\n",
    "    df = df[1:]\n",
    "    \n",
    "    # Fix column name if necessary (assuming first col is Pixel type)\n",
    "    df.columns.values[0] = 'Pixel type'\n",
    "    \n",
    "    # Drop any header row repeats (if any)\n",
    "    df = df[df['Pixel type'] != 'Pixel type']\n",
    "    \n",
    "    cols_to_split = [\n",
    "        'A mean $\\pm$ 1$\\sigma$ [%]',\n",
    "        'k mean $\\pm$ 1$\\sigma$ [s$^{-1}$]',\n",
    "        '$R^2$ mean $\\pm$ 1$\\sigma$'\n",
    "    ]\n",
    "    \n",
    "    for col in cols_to_split:\n",
    "        if col in df.columns:\n",
    "            new_cols = df[col].str.split(r'\\s*\\$\\\\pm\\$\\s*', expand=True)\n",
    "            df[col.replace(' mean $\\pm$ 1$\\sigma$', '_mean')] = new_cols[0].astype(float)\n",
    "            df[col.replace(' mean $\\pm$ 1$\\sigma$', '_std')] = new_cols[1].astype(float)\n",
    "            df = df.drop(columns=[col])\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# Get list of all table files\n",
    "tables = sorted(glob.glob('*_table_np.txt'))\n",
    "\n",
    "# Process all tables and concatenate\n",
    "all_dfs = [process_table(f) for f in tables]\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1805f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_new = combined_df.drop(combined_df.index[5::6])\n",
    "combined_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Your DataFrame is called combined_df and looks like this:\n",
    "# Columns: Pixel type, A_mean [%], A_std [%], ...\n",
    "\n",
    "# Unique pixel types and assign colors\n",
    "pixel_types = combined_df['Pixel type'].unique()[5:6]\n",
    "colors = plt.cm.tab10.colors  # 10 distinct colors\n",
    "color_map = {pt: colors[i % len(colors)] for i, pt in enumerate(pixel_types)}\n",
    "group_size = 5\n",
    "num_groups = len(combined_df) // group_size - 2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "point_spacing = 0.3     # smaller distance between points inside a group\n",
    "group_spacing = 5       # larger distance between groups (files)\n",
    "\n",
    "x_positions = np.arange(num_groups  + 1) * group_spacing\n",
    "\n",
    "# But since pixel_index changes per pixel type, inside the loop:\n",
    "\n",
    "for i, pixel_type in enumerate(pixel_types):\n",
    "    pixel_index = i    \n",
    "    x_pos = x_positions[:, None] + pixel_index * point_spacing  # pixel_index is scalar here\n",
    "    df_pt = combined_df[combined_df['Pixel type'] == pixel_type].reset_index(drop=True)\n",
    "    plt.errorbar(\n",
    "        x_pos,\n",
    "        df_pt['A_mean [%]'].to_numpy(),\n",
    "        yerr=df_pt['A_std [%]'].to_numpy(),\n",
    "        fmt='o',\n",
    "        color=color_map[pixel_type],\n",
    "        label=pixel_type,\n",
    "        capsize=3,\n",
    "        linestyle='None'\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute all upper and lower bounds\n",
    "k_mean = combined_df['A_mean [%]'][5::6].to_numpy()\n",
    "k_std = combined_df['A_std [%]'][5::6].to_numpy()\n",
    "\n",
    "upper_bounds = k_mean + k_std\n",
    "lower_bounds = k_mean - k_std\n",
    "\n",
    "# Find the min of upper bounds and max of lower bounds\n",
    "lowest_upper = np.min(upper_bounds)\n",
    "highest_lower = np.max(lower_bounds)\n",
    "\n",
    "# Highlight the area\n",
    "plt.axhspan(highest_lower, lowest_upper, color='lightblue', alpha=0.6)\n",
    "\n",
    "    \n",
    "plt.xticks(\n",
    "    np.arange(num_groups + 1) * 5, \n",
    "    [f'{table[:19]}' for table in tables],\n",
    "    rotation=45\n",
    ")\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel(f'$A$ mean [\\%]')\n",
    "#plt.title('A mean Â± std per pixel type grouped by file')\n",
    "plt.legend(title='Pixel type')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Amean.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef067e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Your DataFrame is called combined_df and looks like this:\n",
    "# Columns: Pixel type, A_mean [%], A_std [%], ...\n",
    "\n",
    "# Unique pixel types and assign colors\n",
    "pixel_types = combined_df['Pixel type'].unique()[5:6]\n",
    "colors = plt.cm.tab10.colors  # 10 distinct colors\n",
    "color_map = {pt: colors[i % len(colors)] for i, pt in enumerate(pixel_types)}\n",
    "group_size = 5\n",
    "num_groups = len(combined_df) // group_size - 2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "point_spacing = 0.3     # smaller distance between points inside a group\n",
    "group_spacing = 5       # larger distance between groups (files)\n",
    "\n",
    "x_positions = np.arange(num_groups  + 1) * group_spacing\n",
    "\n",
    "# But since pixel_index changes per pixel type, inside the loop:\n",
    "\n",
    "for i, pixel_type in enumerate(pixel_types):\n",
    "    pixel_index = i    \n",
    "    x_pos = x_positions[:, None] + pixel_index * point_spacing  # pixel_index is scalar here\n",
    "    df_pt = combined_df[combined_df['Pixel type'] == pixel_type].reset_index(drop=True)\n",
    "    plt.errorbar(\n",
    "        x_pos,\n",
    "        df_pt['k_mean [s$^{-1}$]'].to_numpy(),\n",
    "        yerr=df_pt['k_std [s$^{-1}$]'].to_numpy(),\n",
    "        fmt='o',\n",
    "        color=color_map[pixel_type],\n",
    "        label=pixel_type,\n",
    "        capsize=3,\n",
    "        linestyle='None'\n",
    "    )\n",
    "\n",
    "\n",
    "plt.xticks(\n",
    "    np.arange(num_groups + 1) * 5, \n",
    "    [f'{table[:19]}' for table in tables],\n",
    "    rotation=45\n",
    ")\n",
    "\n",
    "# Compute all upper and lower bounds\n",
    "k_mean = combined_df['k_mean [s$^{-1}$]'][5::6].to_numpy()\n",
    "k_std = combined_df['k_std [s$^{-1}$]'][5::6].to_numpy()\n",
    "\n",
    "upper_bounds = k_mean + k_std\n",
    "lower_bounds = k_mean - k_std\n",
    "\n",
    "# Find the min of upper bounds and max of lower bounds\n",
    "lowest_upper = np.min(upper_bounds)\n",
    "highest_lower = np.max(lower_bounds)\n",
    "\n",
    "# Highlight the area\n",
    "plt.axhspan(highest_lower, lowest_upper, color='lightblue', alpha=0.6)\n",
    "\n",
    "\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel(f'$k$ mean [s$^{{-1}}$]')\n",
    "#plt.title('A mean Â± std per pixel type grouped by file')\n",
    "plt.legend(title='Pixel type')\n",
    "plt.tight_layout()\n",
    "plt.savefig('kmean.pdf')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
